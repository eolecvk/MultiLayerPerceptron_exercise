{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization of XOR: parity function problem\n",
    "\n",
    "Generalize the previous analysis with functions taking 3 to 10 input variables.  \n",
    "We shall use a dataset to train on the parity function problem:\n",
    "fonction returns 1 when there is an even number of 1-valued attributes; else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run utils/helper_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run utils/preparation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run utils/exploration.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run utils/MLP_utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load attribute dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age [2, 3, 4, 1, 5, 0]\n",
      "menopause [2, 0, 1]\n",
      "tumor_size [2, 6, 5, 4, 7, 1, 0, 3, 8, 10, 9]\n",
      "inv_nodes [0, 4, 2, 5, 6, 3, 1]\n",
      "node_caps [2, 1, 0]\n",
      "deg_malig [2, 0, 1]\n",
      "breast [1, 0]\n",
      "breast_quad [3, 1, 2, 5, 4, 0]\n",
      "irradiat [0, 1]\n",
      "Class [1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Load encoded attribute values from the Breast Cancer dataset\n",
    "fpath = \"/tmp/DM2_attr_val_encoded.json\"\n",
    "attr_val_breast_dataset_encoded = load_json(fpath)\n",
    "    \n",
    "# Preview\n",
    "for k, v in attr_val_breast_dataset_encoded.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then generate sample records randomly and classify them using the parity function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MLP model training pipeline for XOR datasets (3 to 10 features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 attributes:\n",
      "Dataset size: 800\n",
      "Sample records:\n",
      "(4, 1, 1)\n",
      "(1, 1, 1)\n",
      "(1, 6, 10)\n",
      "(1, 1, 1)\n",
      "(1, 1, 5)\n",
      "(1, 0, 6)\n",
      "(1, 1, 4)\n",
      "(1, 5, 4)\n",
      "(2, 2, 10)\n",
      "(2, 5, 0)\n",
      "\n",
      "Sample labels\n",
      "[1, 0, 0, 0, 1, 0, 1, 0, 1, 1]\n",
      "best params [score=0.95625]:\n",
      "{\n",
      "  \"activation\": \"tanh\",\n",
      "  \"alpha\": 0.0020487755102040817,\n",
      "  \"hidden_layer_sizes\": [\n",
      "    14\n",
      "  ],\n",
      "  \"learning_rate\": \"constant\",\n",
      "  \"learning_rate_init\": 0.005353061224489797,\n",
      "  \"max_iter\": 1000,\n",
      "  \"momentum\": 0.4,\n",
      "  \"solver\": \"lbfgs\"\n",
      "}\n",
      "\t---------------------------\n",
      "\n",
      "4 attributes:\n",
      "Dataset size: 800\n",
      "Sample records:\n",
      "(1, 1, 1, 5)\n",
      "(5, 1, 0, 5)\n",
      "(1, 1, 1, 1)\n",
      "(0, 3, 0, 7)\n",
      "(1, 1, 1, 1)\n",
      "(1, 1, 2, 1)\n",
      "(1, 3, 0, 4)\n",
      "(4, 2, 2, 1)\n",
      "(5, 1, 1, 1)\n",
      "(4, 4, 2, 1)\n",
      "\n",
      "Sample labels\n",
      "[0, 0, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "best params [score=0.91125]:\n",
      "{\n",
      "  \"activation\": \"tanh\",\n",
      "  \"alpha\": 0.002456530612244898,\n",
      "  \"hidden_layer_sizes\": [\n",
      "    14,\n",
      "    11\n",
      "  ],\n",
      "  \"learning_rate\": \"invscaling\",\n",
      "  \"learning_rate_init\": 0.0035346938775510208,\n",
      "  \"max_iter\": 1200,\n",
      "  \"momentum\": 1.0,\n",
      "  \"solver\": \"adam\"\n",
      "}\n",
      "\t---------------------------\n",
      "\n",
      "5 attributes:\n",
      "Dataset size: 800\n",
      "Sample records:\n",
      "(1, 1, 2, 1, 1)\n",
      "(1, 5, 0, 1, 1)\n",
      "(1, 2, 2, 1, 0)\n",
      "(5, 3, 0, 0, 0)\n",
      "(4, 4, 0, 5, 0)\n",
      "(5, 4, 0, 4, 0)\n",
      "(1, 1, 1, 6, 1)\n",
      "(1, 1, 1, 1, 1)\n",
      "(4, 1, 2, 1, 1)\n",
      "(4, 1, 1, 1, 1)\n",
      "\n",
      "Sample labels\n",
      "[1, 0, 1, 1, 1, 1, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "for n_attributes in range(3, 11):\n",
    "    \n",
    "    print(\"{} attributes:\".format(n_attributes))\n",
    "    \n",
    "    #print(\"Training for {n_attributes}-attributes XOR dataset:\".format(n_attributes))\n",
    "    \n",
    "    XOR_df = generate_XOR_dataset(n_examples=800,\n",
    "                         attr_dict=attr_val_breast_dataset_encoded,\n",
    "                         n_attributes=n_attributes,\n",
    "                         no_duplicate=False)\n",
    "    \n",
    "    X, y = get_nn_inputs(XOR_df)\n",
    "    \n",
    "    pprint_X_y(X, y)\n",
    "    \n",
    "    best_model = find_best_model(X, y, n_iter=300,\n",
    "                                 max_layers=n_attributes-2,\n",
    "                                 max_neurons=15)\n",
    "    \n",
    "    pprint_best_model(best_model)\n",
    "    print('\\t---------------------------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
