{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization of XOR: parity function problem\n",
    "\n",
    "Generalize the previous analysis with functions taking 3 to 10 input variables.  \n",
    "We shall use a dataset to train on the parity function problem:\n",
    "fonction returns 1 when there is an even number of 1-valued attributes; else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run utils/helper_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run utils/preparation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run utils/exploration.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run utils/MLP_utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load attribute dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age [2, 3, 4, 1, 5, 0]\n",
      "menopause [2, 0, 1]\n",
      "tumor_size [2, 6, 5, 4, 7, 1, 0, 3, 8, 10, 9]\n",
      "inv_nodes [0, 4, 2, 5, 6, 3, 1]\n",
      "node_caps [2, 1, 0]\n",
      "deg_malig [2, 0, 1]\n",
      "breast [1, 0]\n",
      "breast_quad [3, 1, 2, 5, 4, 0]\n",
      "irradiat [0, 1]\n",
      "Class [1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Load encoded attribute values from the Breast Cancer dataset\n",
    "fpath = \"/tmp/DM2_attr_val_encoded.json\"\n",
    "attr_val_breast_dataset_encoded = load_json(fpath)\n",
    "    \n",
    "# Preview\n",
    "for k, v in attr_val_breast_dataset_encoded.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then generate sample records randomly and classify them using the parity function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MLP model training pipeline for XOR datasets (3 to 10 features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def XOR_generate_train(n_attributes):\n",
    "\n",
    "    print(\"{} attributes:\".format(n_attributes))\n",
    "    \n",
    "    #print(\"Training for {n_attributes}-attributes XOR dataset:\".format(n_attributes))\n",
    "    \n",
    "    XOR_df = generate_XOR_dataset(n_examples=800,\n",
    "                         attr_dict=attr_val_breast_dataset_encoded,\n",
    "                         n_attributes=n_attributes,\n",
    "                         no_duplicate=False)\n",
    "    \n",
    "    X, y = get_nn_inputs(XOR_df)\n",
    "    \n",
    "    pprint_X_y(X, y)\n",
    "    \n",
    "    best_model = find_best_model(X, y, n_iter=200,\n",
    "                                 max_layers=n_attributes-1,\n",
    "                                 max_neurons=15)\n",
    "    \n",
    "    pprint_best_model(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for n_attributes in range(3, 11):\n",
    "    \n",
    "#     print(\"{} attributes:\".format(n_attributes))\n",
    "    \n",
    "#     #print(\"Training for {n_attributes}-attributes XOR dataset:\".format(n_attributes))\n",
    "    \n",
    "#     XOR_df = generate_XOR_dataset(n_examples=800,\n",
    "#                          attr_dict=attr_val_breast_dataset_encoded,\n",
    "#                          n_attributes=n_attributes,\n",
    "#                          no_duplicate=False)\n",
    "    \n",
    "#     X, y = get_nn_inputs(XOR_df)\n",
    "    \n",
    "#     pprint_X_y(X, y)\n",
    "    \n",
    "#     best_model = find_best_model(X, y, n_iter=300,\n",
    "#                                  max_layers=n_attributes-1,\n",
    "#                                  max_neurons=15)\n",
    "    \n",
    "#     pprint_best_model(best_model)\n",
    "#     print('\\t---------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 attributes:\n",
      "Dataset size: 800\n",
      "Sample records:\n",
      "(0, 1, 2)\n",
      "(0, 2, 2)\n",
      "(1, 1, 0)\n",
      "(1, 6, 1)\n",
      "(0, 4, 2)\n",
      "(0, 2, 1)\n",
      "(1, 1, 1)\n",
      "(0, 6, 2)\n",
      "(0, 1, 0)\n",
      "(0, 2, 1)\n",
      "\n",
      "Sample labels\n",
      "[0, 1, 1, 1, 1, 0, 0, 1, 0, 0]\n",
      "best params [score=1.0]:\n",
      "{\n",
      "  \"activation\": \"logistic\",\n",
      "  \"alpha\": 0.0073495918367346945,\n",
      "  \"hidden_layer_sizes\": [\n",
      "    12,\n",
      "    13\n",
      "  ],\n",
      "  \"learning_rate\": \"constant\",\n",
      "  \"learning_rate_init\": 0.009393877551020408,\n",
      "  \"max_iter\": 1000,\n",
      "  \"momentum\": 0.9,\n",
      "  \"solver\": \"lbfgs\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "XOR_generate_train(n_attributes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 attributes:\n",
      "Dataset size: 800\n",
      "Sample records:\n",
      "(1, 1, 1, 1)\n",
      "(5, 1, 0, 1)\n",
      "(4, 5, 0, 5)\n",
      "(5, 5, 1, 10)\n",
      "(1, 4, 0, 1)\n",
      "(5, 4, 0, 1)\n",
      "(1, 1, 1, 1)\n",
      "(1, 1, 1, 1)\n",
      "(3, 0, 0, 0)\n",
      "(2, 0, 1, 4)\n",
      "\n",
      "Sample labels\n",
      "[1, 1, 1, 0, 1, 0, 1, 1, 1, 0]\n",
      "best params [score=0.9675]:\n",
      "{\n",
      "  \"activation\": \"tanh\",\n",
      "  \"alpha\": 0.00673795918367347,\n",
      "  \"hidden_layer_sizes\": [\n",
      "    11,\n",
      "    11,\n",
      "    8\n",
      "  ],\n",
      "  \"learning_rate\": \"invscaling\",\n",
      "  \"learning_rate_init\": 0.008383673469387756,\n",
      "  \"max_iter\": 1000,\n",
      "  \"momentum\": 0.1,\n",
      "  \"solver\": \"adam\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "XOR_generate_train(n_attributes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 attributes:\n",
      "Dataset size: 800\n",
      "Sample records:\n",
      "(1, 1, 1, 1, 1)\n",
      "(0, 2, 1, 0, 1)\n",
      "(0, 0, 0, 2, 8)\n",
      "(1, 2, 0, 1, 1)\n",
      "(0, 1, 0, 0, 1)\n",
      "(1, 1, 6, 1, 1)\n",
      "(0, 1, 6, 2, 4)\n",
      "(0, 2, 5, 2, 10)\n",
      "(1, 1, 1, 1, 1)\n",
      "(0, 1, 4, 0, 2)\n",
      "\n",
      "Sample labels\n",
      "[0, 1, 1, 0, 1, 1, 0, 1, 0, 0]\n",
      "best params [score=0.86375]:\n",
      "{\n",
      "  \"activation\": \"tanh\",\n",
      "  \"alpha\": 0.01,\n",
      "  \"hidden_layer_sizes\": [\n",
      "    15,\n",
      "    15,\n",
      "    7,\n",
      "    4\n",
      "  ],\n",
      "  \"learning_rate\": \"invscaling\",\n",
      "  \"learning_rate_init\": 0.002322448979591837,\n",
      "  \"max_iter\": 1200,\n",
      "  \"momentum\": 0.7000000000000001,\n",
      "  \"solver\": \"adam\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "XOR_generate_train(n_attributes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 attributes:\n",
      "Dataset size: 800\n",
      "Sample records:\n",
      "(1, 1, 1, 3, 1, 2)\n",
      "(1, 0, 2, 6, 0, 2)\n",
      "(0, 1, 1, 1, 1, 1)\n",
      "(1, 0, 3, 1, 0, 1)\n",
      "(5, 1, 0, 3, 0, 0)\n",
      "(5, 0, 1, 1, 1, 0)\n",
      "(1, 1, 1, 1, 1, 0)\n",
      "(1, 1, 1, 1, 1, 2)\n",
      "(1, 0, 0, 3, 1, 0)\n",
      "(1, 0, 1, 1, 1, 1)\n",
      "\n",
      "Sample labels\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "best params [score=0.78875]:\n",
      "{\n",
      "  \"activation\": \"tanh\",\n",
      "  \"alpha\": 0.0077573469387755105,\n",
      "  \"hidden_layer_sizes\": [\n",
      "    12,\n",
      "    9,\n",
      "    15,\n",
      "    13,\n",
      "    3\n",
      "  ],\n",
      "  \"learning_rate\": \"adaptive\",\n",
      "  \"learning_rate_init\": 0.0077775510204081645,\n",
      "  \"max_iter\": 1200,\n",
      "  \"momentum\": 0.7000000000000001,\n",
      "  \"solver\": \"lbfgs\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "XOR_generate_train(n_attributes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 attributes:\n",
      "Dataset size: 800\n",
      "Sample records:\n",
      "(4, 0, 1, 1, 0, 1, 1)\n",
      "(4, 1, 4, 4, 1, 0, 10)\n",
      "(2, 0, 3, 1, 1, 2, 0)\n",
      "(0, 0, 5, 2, 1, 2, 10)\n",
      "(4, 0, 1, 2, 0, 0, 10)\n",
      "(2, 0, 4, 1, 1, 2, 8)\n",
      "(1, 1, 1, 1, 1, 0, 1)\n",
      "(2, 0, 0, 5, 0, 1, 3)\n",
      "(3, 0, 2, 1, 1, 1, 3)\n",
      "(1, 0, 0, 0, 0, 1, 9)\n",
      "\n",
      "Sample labels\n",
      "[1, 1, 1, 0, 0, 1, 1, 0, 0, 1]\n",
      "best params [score=0.69]:\n",
      "{\n",
      "  \"activation\": \"tanh\",\n",
      "  \"alpha\": 0.008165102040816326,\n",
      "  \"hidden_layer_sizes\": [\n",
      "    9,\n",
      "    14,\n",
      "    12,\n",
      "    5,\n",
      "    15,\n",
      "    15\n",
      "  ],\n",
      "  \"learning_rate\": \"constant\",\n",
      "  \"learning_rate_init\": 0.008585714285714287,\n",
      "  \"max_iter\": 1000,\n",
      "  \"momentum\": 0.9,\n",
      "  \"solver\": \"adam\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "XOR_generate_train(n_attributes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 attributes:\n",
      "Dataset size: 800\n",
      "Sample records:\n",
      "(0, 2, 1, 2, 1, 1, 0, 1)\n",
      "(0, 1, 1, 1, 1, 1, 1, 1)\n",
      "(0, 2, 1, 0, 0, 0, 2, 6)\n",
      "(0, 1, 2, 1, 0, 1, 2, 1)\n",
      "(1, 1, 2, 1, 0, 1, 1, 1)\n",
      "(0, 5, 1, 1, 0, 1, 0, 6)\n",
      "(1, 1, 1, 1, 1, 1, 0, 1)\n",
      "(1, 3, 1, 1, 0, 0, 2, 1)\n",
      "(0, 0, 2, 6, 0, 2, 0, 10)\n",
      "(0, 2, 2, 4, 0, 0, 0, 1)\n",
      "\n",
      "Sample labels\n",
      "[1, 0, 0, 1, 1, 0, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Kernel crash on jupyter notebook; need to tune these down\n",
    "# XOR_generate_train(n_attributes=8)\n",
    "# XOR_generate_train(n_attributes=9)\n",
    "# XOR_generate_train(n_attributes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
